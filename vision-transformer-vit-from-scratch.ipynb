{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-02-19T19:29:02.430655Z",
     "iopub.status.busy": "2021-02-19T19:29:02.429990Z",
     "iopub.status.idle": "2021-02-19T19:29:08.407351Z",
     "shell.execute_reply": "2021-02-19T19:29:08.408264Z"
    },
    "papermill": {
     "duration": 6.000267,
     "end_time": "2021-02-19T19:29:08.408597",
     "exception": false,
     "start_time": "2021-02-19T19:29:02.408330",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-01 12:58:12.710815: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version 2.9.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow_addons as tfa\n",
    "import glob, random, os, warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "print('TensorFlow Version ' + tf.__version__)\n",
    "\n",
    "def seed_everything(seed = 0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "seed_everything()\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a csv with image name and labels from a particular directory \n",
    "def create_csv(path, csv_name):\n",
    "    image_names = []\n",
    "    labels = []\n",
    "    for fol in os.listdir(path):\n",
    "        for i in os.listdir(path+'/'+fol):\n",
    "            labels.append(fol)\n",
    "            image_names.append(i)\n",
    "    df = pd.DataFrame({'image_id': image_names, 'label': labels})\n",
    "    df.to_csv(csv_name, index=False)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13_60.jpg</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12_32.jpg</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13_38.jpg</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13_39.jpg</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13_67.jpg</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>8_66.jpg</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>8_41.jpg</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>8_35.jpg</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>8_70.jpg</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>8_20.jpg</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>808 rows ร 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      image_id label\n",
       "0    13_60.jpg     8\n",
       "1    12_32.jpg     8\n",
       "2    13_38.jpg     8\n",
       "3    13_39.jpg     8\n",
       "4    13_67.jpg     8\n",
       "..         ...   ...\n",
       "803   8_66.jpg     6\n",
       "804   8_41.jpg     6\n",
       "805   8_35.jpg     6\n",
       "806   8_70.jpg     6\n",
       "807   8_20.jpg     6\n",
       "\n",
       "[808 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_csv('/home/abhaylal/Desktop/Projects/GAIT/train', 'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-19T19:29:08.468151Z",
     "iopub.status.busy": "2021-02-19T19:29:08.467446Z",
     "iopub.status.idle": "2021-02-19T19:29:08.514463Z",
     "shell.execute_reply": "2021-02-19T19:29:08.517226Z"
    },
    "papermill": {
     "duration": 0.084564,
     "end_time": "2021-02-19T19:29:08.517368",
     "exception": false,
     "start_time": "2021-02-19T19:29:08.432804",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_height=360\n",
    "img_width=480\n",
    "image_size=360\n",
    "batch_size = 16\n",
    "n_classes = 10\n",
    "\n",
    "train_path = '/home/abhaylal/Desktop/Projects/GAIT/train/'\n",
    "test_path = '/home/abhaylal/Desktop/Projects/GAIT/test/'\n",
    "\n",
    "df_train = pd.read_csv('/home/abhaylal/Desktop/Projects/GAIT/Paper-2/train.csv', dtype = 'str')\n",
    "\n",
    "test_images = glob.glob(test_path + '/*.jpg')\n",
    "df_test = pd.DataFrame(test_images, columns = ['image_path'])\n",
    "\n",
    "classes = {0 : \"Person 1\",\n",
    "           1 : \"Person 2\",\n",
    "           2 : \"Person 3\",\n",
    "           3 : \"Person 4\",\n",
    "           4 : \"Person 5\",\n",
    "           5 : \"Person 6\",\n",
    "           6 : \"Person 7\",\n",
    "           7 : \"Person 8\",\n",
    "           8 : \"Person 9\",\n",
    "           9 : \"Person 10\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.023196,
     "end_time": "2021-02-19T19:29:08.572755",
     "exception": false,
     "start_time": "2021-02-19T19:29:08.549559",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-19T19:29:08.611026Z",
     "iopub.status.busy": "2021-02-19T19:29:08.610455Z",
     "iopub.status.idle": "2021-02-19T19:29:08.613608Z",
     "shell.execute_reply": "2021-02-19T19:29:08.613185Z"
    },
    "papermill": {
     "duration": 0.025305,
     "end_time": "2021-02-19T19:29:08.613729",
     "exception": false,
     "start_time": "2021-02-19T19:29:08.588424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_augment(image):\n",
    "    p_spatial = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n",
    "    p_rotate = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n",
    " \n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    \n",
    "    if p_spatial > .75:\n",
    "        image = tf.image.transpose(image)\n",
    "        \n",
    "    # Rotates\n",
    "    if p_rotate > .75:\n",
    "        image = tf.image.rot90(image, k = 3) # rotate 270ยบ\n",
    "    elif p_rotate > .5:\n",
    "        image = tf.image.rot90(image, k = 2) # rotate 180ยบ\n",
    "    elif p_rotate > .25:\n",
    "        image = tf.image.rot90(image, k = 1) # rotate 90ยบ\n",
    "        \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016355,
     "end_time": "2021-02-19T19:29:08.645660",
     "exception": false,
     "start_time": "2021-02-19T19:29:08.629305",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-19T19:29:08.691788Z",
     "iopub.status.busy": "2021-02-19T19:29:08.691008Z",
     "iopub.status.idle": "2021-02-19T19:29:31.216689Z",
     "shell.execute_reply": "2021-02-19T19:29:31.216156Z"
    },
    "papermill": {
     "duration": 22.555326,
     "end_time": "2021-02-19T19:29:31.216831",
     "exception": false,
     "start_time": "2021-02-19T19:29:08.661505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 validated image filenames belonging to 0 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\ntest_gen = datagen.flow_from_dataframe(dataframe = df_test,\\n                                       x_col = 'image_path',\\n                                       y_col = None,\\n                                       batch_size = batch_size,\\n                                       seed = 1,\\n                                       color_mode = 'rgb',\\n                                       shuffle = False,\\n                                       class_mode = None)\\n\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(samplewise_center = True,\n",
    "                                                          samplewise_std_normalization = True,\n",
    "                                                          validation_split = 0.2,\n",
    "                                                          preprocessing_function = data_augment)\n",
    "\n",
    "train_gen = datagen.flow_from_dataframe(dataframe = df_train,\n",
    "                                        directory = train_path,\n",
    "                                        x_col = 'image_id',\n",
    "                                        y_col = 'label',\n",
    "                                        batch_size = batch_size,\n",
    "                                        seed = 1,\n",
    "                                        class_mode = 'categorical')\n",
    "\"\"\"\n",
    "test_gen = datagen.flow_from_dataframe(dataframe = df_test,\n",
    "                                       x_col = 'image_path',\n",
    "                                       y_col = None,\n",
    "                                       batch_size = batch_size,\n",
    "                                       seed = 1,\n",
    "                                       color_mode = 'rgb',\n",
    "                                       shuffle = False,\n",
    "                                       class_mode = None)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018907,
     "end_time": "2021-02-19T19:29:31.255514",
     "exception": false,
     "start_time": "2021-02-19T19:29:31.236607",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Sample Images Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 360"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.034164,
     "end_time": "2021-02-19T19:29:38.679778",
     "exception": false,
     "start_time": "2021-02-19T19:29:38.645614",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-19T19:29:38.753954Z",
     "iopub.status.busy": "2021-02-19T19:29:38.752672Z",
     "iopub.status.idle": "2021-02-19T19:29:38.755129Z",
     "shell.execute_reply": "2021-02-19T19:29:38.755558Z"
    },
    "papermill": {
     "duration": 0.042212,
     "end_time": "2021-02-19T19:29:38.755689",
     "exception": false,
     "start_time": "2021-02-19T19:29:38.713477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "num_epochs = 1\n",
    "\n",
    "patch_size = 7  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [56, 28]  # Size of the dense layers of the final classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.033317,
     "end_time": "2021-02-19T19:29:38.822264",
     "exception": false,
     "start_time": "2021-02-19T19:29:38.788947",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Building the Model and it's Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.033589,
     "end_time": "2021-02-19T19:29:38.889413",
     "exception": false,
     "start_time": "2021-02-19T19:29:38.855824",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Multilayer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-19T19:29:38.961393Z",
     "iopub.status.busy": "2021-02-19T19:29:38.960782Z",
     "iopub.status.idle": "2021-02-19T19:29:38.964822Z",
     "shell.execute_reply": "2021-02-19T19:29:38.964389Z"
    },
    "papermill": {
     "duration": 0.042068,
     "end_time": "2021-02-19T19:29:38.964931",
     "exception": false,
     "start_time": "2021-02-19T19:29:38.922863",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = L.Dense(units, activation = tf.nn.gelu)(x)\n",
    "        x = L.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03389,
     "end_time": "2021-02-19T19:29:39.032151",
     "exception": false,
     "start_time": "2021-02-19T19:29:38.998261",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Patch Creation Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-19T19:29:39.109321Z",
     "iopub.status.busy": "2021-02-19T19:29:39.108149Z",
     "iopub.status.idle": "2021-02-19T19:29:39.110569Z",
     "shell.execute_reply": "2021-02-19T19:29:39.110972Z"
    },
    "papermill": {
     "duration": 0.044558,
     "end_time": "2021-02-19T19:29:39.111101",
     "exception": false,
     "start_time": "2021-02-19T19:29:39.066543",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Patches(L.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images = images,\n",
    "            sizes = [1, self.patch_size, self.patch_size, 1],\n",
    "            strides = [1, self.patch_size, self.patch_size, 1],\n",
    "            rates = [1, 1, 1, 1],\n",
    "            padding = 'VALID',\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.033642,
     "end_time": "2021-02-19T19:29:39.178614",
     "exception": false,
     "start_time": "2021-02-19T19:29:39.144972",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Sample Image Patches Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.037184,
     "end_time": "2021-02-19T19:30:19.677554",
     "exception": false,
     "start_time": "2021-02-19T19:30:19.640370",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Patch Encoding Layer\n",
    "The `PatchEncoder` layer will linearly transform a patch by projecting it into a vector of size `projection_dim`. In addition, it adds a learnable position embedding to the projected vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-19T19:30:19.759791Z",
     "iopub.status.busy": "2021-02-19T19:30:19.759088Z",
     "iopub.status.idle": "2021-02-19T19:30:19.762778Z",
     "shell.execute_reply": "2021-02-19T19:30:19.762354Z"
    },
    "papermill": {
     "duration": 0.047305,
     "end_time": "2021-02-19T19:30:19.762888",
     "exception": false,
     "start_time": "2021-02-19T19:30:19.715583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PatchEncoder(L.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = L.Dense(units = projection_dim)\n",
    "        self.position_embedding = L.Embedding(\n",
    "            input_dim = num_patches, output_dim = projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start = 0, limit = self.num_patches, delta = 1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.037551,
     "end_time": "2021-02-19T19:30:19.837807",
     "exception": false,
     "start_time": "2021-02-19T19:30:19.800256",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Build the ViT model\n",
    "The ViT model consists of multiple Transformer blocks, which use the `MultiHeadAttention` layer as a self-attention mechanism applied to the sequence of patches. The Transformer blocks produce a `[batch_size, num_patches, projection_dim]` tensor, which is processed via an classifier head with softmax to produce the final class probabilities output.\n",
    "\n",
    "Unlike the technique described in the paper, which prepends a learnable embedding to the sequence of encoded patches to serve as the image representation, all the outputs of the final Transformer block are reshaped with `Flatten()` and used as the image representation input to the classifier head. Note that the `GlobalAveragePooling1D` layer could also be used instead to aggregate the outputs of the Transformer block, especially when the number of patches and the projection dimensions are large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-19T19:30:19.922409Z",
     "iopub.status.busy": "2021-02-19T19:30:19.921660Z",
     "iopub.status.idle": "2021-02-19T19:30:19.924571Z",
     "shell.execute_reply": "2021-02-19T19:30:19.924139Z"
    },
    "papermill": {
     "duration": 0.04892,
     "end_time": "2021-02-19T19:30:19.924685",
     "exception": false,
     "start_time": "2021-02-19T19:30:19.875765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def vision_transformer():\n",
    "    inputs = L.Input(shape = (image_size, image_size, 3))\n",
    "    \n",
    "    # Create patches.\n",
    "    patches = Patches(patch_size)(inputs)\n",
    "    \n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        \n",
    "        # Layer normalization 1.\n",
    "        x1 = L.LayerNormalization(epsilon = 1e-6)(encoded_patches)\n",
    "        \n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = L.MultiHeadAttention(\n",
    "            num_heads = num_heads, key_dim = projection_dim, dropout = 0.1\n",
    "        )(x1, x1)\n",
    "        \n",
    "        # Skip connection 1.\n",
    "        x2 = L.Add()([attention_output, encoded_patches])\n",
    "        \n",
    "        # Layer normalization 2.\n",
    "        x3 = L.LayerNormalization(epsilon = 1e-6)(x2)\n",
    "        \n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units = transformer_units, dropout_rate = 0.1)\n",
    "        \n",
    "        # Skip connection 2.\n",
    "        encoded_patches = L.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = L.LayerNormalization(epsilon = 1e-6)(encoded_patches)\n",
    "    representation = L.Flatten()(representation)\n",
    "    representation = L.Dropout(0.5)(representation)\n",
    "    \n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units = mlp_head_units, dropout_rate = 0.5)\n",
    "    \n",
    "    # Classify outputs.\n",
    "    logits = L.Dense(n_classes)(features)\n",
    "    \n",
    "    # Create the model.\n",
    "    model = tf.keras.Model(inputs = inputs, outputs = logits)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-19T19:30:20.004894Z",
     "iopub.status.busy": "2021-02-19T19:30:20.004175Z",
     "iopub.status.idle": "2021-02-19T19:30:20.007032Z",
     "shell.execute_reply": "2021-02-19T19:30:20.006636Z"
    },
    "papermill": {
     "duration": 0.044824,
     "end_time": "2021-02-19T19:30:20.007142",
     "exception": false,
     "start_time": "2021-02-19T19:30:19.962318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "decay_steps = train_gen.n // train_gen.batch_size\n",
    "initial_learning_rate = learning_rate\n",
    "\n",
    "lr_decayed_fn = tf.keras.experimental.CosineDecay(initial_learning_rate, decay_steps)\n",
    "\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_decayed_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-19T19:30:20.093765Z",
     "iopub.status.busy": "2021-02-19T19:30:20.092926Z",
     "iopub.status.idle": "2021-02-19T19:42:04.893702Z",
     "shell.execute_reply": "2021-02-19T19:42:04.892360Z"
    },
    "papermill": {
     "duration": 704.849094,
     "end_time": "2021-02-19T19:42:04.893834",
     "exception": false,
     "start_time": "2021-02-19T19:30:20.044740",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Asked to retrieve element 0, but the Sequence has length 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/abhaylal/Desktop/Projects/GAIT/Paper-2/vision-transformer-vit-from-scratch.ipynb Cell 24\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/abhaylal/Desktop/Projects/GAIT/Paper-2/vision-transformer-vit-from-scratch.ipynb#X34sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m checkpointer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mModelCheckpoint(filepath \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m./model.hdf5\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/abhaylal/Desktop/Projects/GAIT/Paper-2/vision-transformer-vit-from-scratch.ipynb#X34sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m                                                   monitor \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/abhaylal/Desktop/Projects/GAIT/Paper-2/vision-transformer-vit-from-scratch.ipynb#X34sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m                                                   verbose \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/abhaylal/Desktop/Projects/GAIT/Paper-2/vision-transformer-vit-from-scratch.ipynb#X34sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m                                                   save_best_only \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/abhaylal/Desktop/Projects/GAIT/Paper-2/vision-transformer-vit-from-scratch.ipynb#X34sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m                                                   save_weights_only \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/abhaylal/Desktop/Projects/GAIT/Paper-2/vision-transformer-vit-from-scratch.ipynb#X34sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m                                                   mode \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/abhaylal/Desktop/Projects/GAIT/Paper-2/vision-transformer-vit-from-scratch.ipynb#X34sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m callbacks \u001b[39m=\u001b[39m [earlystopping, lr_scheduler, checkpointer]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/abhaylal/Desktop/Projects/GAIT/Paper-2/vision-transformer-vit-from-scratch.ipynb#X34sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(x \u001b[39m=\u001b[39;49m train_gen,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/abhaylal/Desktop/Projects/GAIT/Paper-2/vision-transformer-vit-from-scratch.ipynb#X34sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m           steps_per_epoch \u001b[39m=\u001b[39;49m STEP_SIZE_TRAIN,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/abhaylal/Desktop/Projects/GAIT/Paper-2/vision-transformer-vit-from-scratch.ipynb#X34sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m           \u001b[39m#validation_data = valid_gen,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/abhaylal/Desktop/Projects/GAIT/Paper-2/vision-transformer-vit-from-scratch.ipynb#X34sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m           \u001b[39m#validation_steps = STEP_SIZE_VALID,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/abhaylal/Desktop/Projects/GAIT/Paper-2/vision-transformer-vit-from-scratch.ipynb#X34sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m           epochs \u001b[39m=\u001b[39;49m num_epochs,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/abhaylal/Desktop/Projects/GAIT/Paper-2/vision-transformer-vit-from-scratch.ipynb#X34sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m           callbacks \u001b[39m=\u001b[39;49m callbacks)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/preprocessing/image.py:100\u001b[0m, in \u001b[0;36mIterator.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[1;32m     99\u001b[0m   \u001b[39mif\u001b[39;00m idx \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 100\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAsked to retrieve element \u001b[39m\u001b[39m{idx}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    101\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39mbut the Sequence \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    102\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39mhas length \u001b[39m\u001b[39m{length}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(idx\u001b[39m=\u001b[39midx, length\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m)))\n\u001b[1;32m    103\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseed \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mseed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseed \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal_batches_seen)\n",
      "\u001b[0;31mValueError\u001b[0m: Asked to retrieve element 0, but the Sequence has length 0"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "\n",
    "model = vision_transformer()\n",
    "    \n",
    "model.compile(optimizer = optimizer, \n",
    "              loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing = 0.1), \n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "STEP_SIZE_TRAIN = train_gen.n // train_gen.batch_size\n",
    "#STEP_SIZE_VALID = test_gen.n // valid_gen.batch_size\n",
    "\n",
    "earlystopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy',\n",
    "                                                 min_delta = 1e-4,\n",
    "                                                 patience = 5,\n",
    "                                                 mode = 'max',\n",
    "                                                 restore_best_weights = True,\n",
    "                                                 verbose = 1)\n",
    "\n",
    "checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath = './model.hdf5',\n",
    "                                                  monitor = 'val_accuracy', \n",
    "                                                  verbose = 1, \n",
    "                                                  save_best_only = True,\n",
    "                                                  save_weights_only = True,\n",
    "                                                  mode = 'max')\n",
    "\n",
    "callbacks = [earlystopping, lr_scheduler, checkpointer]\n",
    "\n",
    "model.fit(x = train_gen,\n",
    "          steps_per_epoch = STEP_SIZE_TRAIN,\n",
    "          #validation_data = valid_gen,\n",
    "          #validation_steps = STEP_SIZE_VALID,\n",
    "          epochs = num_epochs,\n",
    "          callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.311801,
     "end_time": "2021-02-19T19:42:05.516973",
     "exception": false,
     "start_time": "2021-02-19T19:42:05.205172",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-19T19:42:06.153327Z",
     "iopub.status.busy": "2021-02-19T19:42:06.152713Z",
     "iopub.status.idle": "2021-02-19T19:48:18.582341Z",
     "shell.execute_reply": "2021-02-19T19:48:18.579890Z"
    },
    "papermill": {
     "duration": 372.747579,
     "end_time": "2021-02-19T19:48:18.582474",
     "exception": false,
     "start_time": "2021-02-19T19:42:05.834895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training results\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/abhaylal/Desktop/Projects/GAIT/Paper-2/vision-transformer-vit-from-scratch.ipynb Cell 30\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/abhaylal/Desktop/Projects/GAIT/Paper-2/vision-transformer-vit-from-scratch.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTraining results\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/abhaylal/Desktop/Projects/GAIT/Paper-2/vision-transformer-vit-from-scratch.ipynb#X35sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39mevaluate(train_gen)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/abhaylal/Desktop/Projects/GAIT/Paper-2/vision-transformer-vit-from-scratch.ipynb#X35sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mValidation results\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/abhaylal/Desktop/Projects/GAIT/Paper-2/vision-transformer-vit-from-scratch.ipynb#X35sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model\u001b[39m.\u001b[39mevaluate(valid_gen)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "print('Training results')\n",
    "model.evaluate(train_gen)\n",
    "\n",
    "print('Validation results')\n",
    "model.evaluate(valid_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.996689,
     "end_time": "2021-02-19T19:48:20.368309",
     "exception": false,
     "start_time": "2021-02-19T19:48:19.371620",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Summary\n",
    "\n",
    "Note that the state of the art results reported in the paper are achieved by pre-training the ViT model using the JFT-300M dataset, then fine-tuning it on the target dataset. To improve the model quality without pre-training, you can try to train the model for more epochs, use a larger number of Transformer layers, resize the input images, change the patch size, or increase the projection dimensions. Besides, as mentioned in the paper, the quality of the model is affected not only by architecture choices, but also by parameters such as the learning rate schedule, optimizer, weight decay, etc. In practice, it's recommended to fine-tune a ViT model that was pre-trained using a large, high-resolution dataset. <br>\n",
    "\n",
    "**References:** <br>\n",
    "Keras Docs: https://keras.io/api/ <br>\n",
    "Research Paper: https://arxiv.org/pdf/2010.11929.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1170.103344,
   "end_time": "2021-02-19T19:48:27.401496",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-02-19T19:28:57.298152",
   "version": "2.2.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
